{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"hf://datasets/owaiskha9654/PubMed_MultiLabel_Text_Classification_Dataset_MeSH/PubMed Multi Label Text Classification Dataset Processed.csv\")\n",
    "df = df.sample(n=5000, random_state=42) # for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>abstractText</th>\n",
       "      <th>meshMajor</th>\n",
       "      <th>pmid</th>\n",
       "      <th>meshid</th>\n",
       "      <th>meshroot</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>I</th>\n",
       "      <th>J</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "      <th>N</th>\n",
       "      <th>Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33553</th>\n",
       "      <td>Vertical trauma: injuries to patients who fall...</td>\n",
       "      <td>We reviewed the patterns of injuries sustained...</td>\n",
       "      <td>['Accidental Falls', 'Accidents', 'Adolescent'...</td>\n",
       "      <td>2916780</td>\n",
       "      <td>[['N06.850.135.122'], ['N06.850.135'], ['M01.0...</td>\n",
       "      <td>['Health Care [N]', 'Named Groups [M]', 'Organ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9427</th>\n",
       "      <td>The influence of bilateral sagittal split ramu...</td>\n",
       "      <td>The effect of orthodontic-surgical treatment o...</td>\n",
       "      <td>['Adolescent', 'Adult', 'Chin', 'Esthetics, De...</td>\n",
       "      <td>24946129</td>\n",
       "      <td>[['M01.060.057'], ['M01.060.116'], ['A01.456.5...</td>\n",
       "      <td>['Named Groups [M]', 'Anatomy [A]', 'Analytica...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Altered metabolic incorporation of fucose and ...</td>\n",
       "      <td>Sciatic nerves of 25-week-old genetically diab...</td>\n",
       "      <td>['Animals', 'Carbon Radioisotopes', 'Diabetes ...</td>\n",
       "      <td>6888648</td>\n",
       "      <td>[['B01.050'], ['D01.268.150.075.328', 'D01.496...</td>\n",
       "      <td>['Organisms [B]', 'Chemicals and Drugs [D]', '...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12447</th>\n",
       "      <td>Higher temperatures increase developmental rat...</td>\n",
       "      <td>Effects of temperature on development of Raja ...</td>\n",
       "      <td>['Adaptation, Biological', 'Analysis of Varian...</td>\n",
       "      <td>31049955</td>\n",
       "      <td>[['G16.012'], ['E05.318.740.150', 'N05.715.360...</td>\n",
       "      <td>['Phenomena and Processes [G]', 'Analytical, D...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39489</th>\n",
       "      <td>MR imaging of the flexed knee: comparison to t...</td>\n",
       "      <td>The aim of this study was to obtain MR images ...</td>\n",
       "      <td>['Adolescent', 'Adult', 'Arthroscopy', 'Female...</td>\n",
       "      <td>11097414</td>\n",
       "      <td>[['M01.060.057'], ['M01.060.116'], ['E01.370.3...</td>\n",
       "      <td>['Named Groups [M]', 'Analytical, Diagnostic a...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Title  \\\n",
       "33553  Vertical trauma: injuries to patients who fall...   \n",
       "9427   The influence of bilateral sagittal split ramu...   \n",
       "199    Altered metabolic incorporation of fucose and ...   \n",
       "12447  Higher temperatures increase developmental rat...   \n",
       "39489  MR imaging of the flexed knee: comparison to t...   \n",
       "\n",
       "                                            abstractText  \\\n",
       "33553  We reviewed the patterns of injuries sustained...   \n",
       "9427   The effect of orthodontic-surgical treatment o...   \n",
       "199    Sciatic nerves of 25-week-old genetically diab...   \n",
       "12447  Effects of temperature on development of Raja ...   \n",
       "39489  The aim of this study was to obtain MR images ...   \n",
       "\n",
       "                                               meshMajor      pmid  \\\n",
       "33553  ['Accidental Falls', 'Accidents', 'Adolescent'...   2916780   \n",
       "9427   ['Adolescent', 'Adult', 'Chin', 'Esthetics, De...  24946129   \n",
       "199    ['Animals', 'Carbon Radioisotopes', 'Diabetes ...   6888648   \n",
       "12447  ['Adaptation, Biological', 'Analysis of Varian...  31049955   \n",
       "39489  ['Adolescent', 'Adult', 'Arthroscopy', 'Female...  11097414   \n",
       "\n",
       "                                                  meshid  \\\n",
       "33553  [['N06.850.135.122'], ['N06.850.135'], ['M01.0...   \n",
       "9427   [['M01.060.057'], ['M01.060.116'], ['A01.456.5...   \n",
       "199    [['B01.050'], ['D01.268.150.075.328', 'D01.496...   \n",
       "12447  [['G16.012'], ['E05.318.740.150', 'N05.715.360...   \n",
       "39489  [['M01.060.057'], ['M01.060.116'], ['E01.370.3...   \n",
       "\n",
       "                                                meshroot  A  B  C  D  E  F  G  \\\n",
       "33553  ['Health Care [N]', 'Named Groups [M]', 'Organ...  0  1  1  0  0  1  0   \n",
       "9427   ['Named Groups [M]', 'Anatomy [A]', 'Analytica...  1  1  0  0  1  0  0   \n",
       "199    ['Organisms [B]', 'Chemicals and Drugs [D]', '...  1  1  1  1  1  0  0   \n",
       "12447  ['Phenomena and Processes [G]', 'Analytical, D...  0  1  0  0  1  0  1   \n",
       "39489  ['Named Groups [M]', 'Analytical, Diagnostic a...  1  1  1  0  1  0  1   \n",
       "\n",
       "       H  I  J  L  M  N  Z  \n",
       "33553  0  1  0  0  1  1  0  \n",
       "9427   0  0  0  0  1  1  0  \n",
       "199    0  0  0  0  0  0  0  \n",
       "12447  0  0  0  0  0  1  1  \n",
       "39489  0  0  0  0  1  1  0  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = old_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (749 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "categories = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'L', 'M', 'N', 'Z']\n",
    "curU = 'A'\n",
    "#remove curU from all_categories\n",
    "categories.remove(curU)\n",
    "\n",
    "#make df have only Title, abstractText, curU, and every category in categories\n",
    "df = df[['Title', 'abstractText', curU] + categories]\n",
    "#condense Title, abstractText into just \"text\"\n",
    "df['text'] = df['Title'] + ' ' + df['abstractText']\n",
    "df = df.drop(['Title', 'abstractText'], axis=1)\n",
    "#tokenize text\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "df['text'] = df['text'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "df.head()\n",
    "#make a column \"feature\" that is [text, 'A', 'B', 'C', ...]\n",
    "df['feature'] = df.apply(lambda x: [x['text']] + x[categories].values.tolist(), axis=1)\n",
    "df = df.drop(categories, axis=1)\n",
    "df = df.drop(['text'], axis=1)\n",
    "#rename curU to \"label\"\n",
    "df = df.rename(columns={curU: 'label'})\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()\n",
    "\n",
    "'''#split into train and test without using sklearn\n",
    "train_size = 0.8\n",
    "train_df = df.sample(frac=train_size, random_state=200)\n",
    "test_df = df.drop(train_df.index)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "'''\n",
    "\n",
    "#add validation\n",
    "train_size = 0.7\n",
    "val_size = 0.15\n",
    "\n",
    "train_df = df.sample(frac=train_size, random_state=200)\n",
    "remaining_df = df.drop(train_df.index)\n",
    "val_df = remaining_df.sample(frac=val_size / (1 - train_size), random_state=200)\n",
    "test_df = remaining_df.drop(val_df.index)\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.features = dataframe['feature'].values\n",
    "        self.labels = dataframe['label'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        full_feature = self.features[idx]\n",
    "        text = torch.tensor(full_feature[0], dtype=torch.long)  # Tokenized text\n",
    "        categories = torch.tensor(full_feature[1:], dtype=torch.float)  # Other categories\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float)  # Binary label\n",
    "        return text, categories, label\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts, categories, labels = zip(*batch)\n",
    "    texts_padded = pad_sequence(texts, batch_first=True, padding_value=0)  # Pad texts\n",
    "    categories = torch.stack(categories)  # Stack category vectors\n",
    "    labels = torch.stack(labels).unsqueeze(1)  # Stack labels and add dimension\n",
    "    return texts_padded, categories, labels\n",
    "\n",
    "\n",
    "train_dataset = TextDataset(train_df)\n",
    "val_dataset = TextDataset(val_df)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_filters, kernel_sizes, dense_dim, category_dim):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=k)\n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "        self.category_fc = nn.Linear(category_dim, dense_dim)  # Process category features\n",
    "        self.fc1 = nn.Linear(num_filters * len(kernel_sizes) + dense_dim, 1)  # Combine features\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, categories):\n",
    "        # Process tokenized text\n",
    "        embedded = self.embedding(text).permute(0, 2, 1)  # [batch_size, embedding_dim, seq_len]\n",
    "        conv_outs = [F.relu(conv(embedded)).max(dim=2)[0] for conv in self.convs]\n",
    "        text_features = torch.cat(conv_outs, dim=1)\n",
    "\n",
    "        # Process categorical features\n",
    "        category_features = F.relu(self.category_fc(categories))\n",
    "\n",
    "        # Combine and classify\n",
    "        combined_features = torch.cat([text_features, category_features], dim=1)\n",
    "        output = self.sigmoid(self.fc1(combined_features))\n",
    "        return output\n",
    "\n",
    "# Model Parameters\n",
    "vocab_size = tokenizer.vocab_size\n",
    "embedding_dim = 128\n",
    "num_filters = 100\n",
    "kernel_sizes = [3, 4, 5]\n",
    "dense_dim = 32\n",
    "epochs = 5\n",
    "category_dim = len(df['feature'][0]) - 1  # Number of category features (excluding text)\n",
    "\n",
    "model = TextCNN(vocab_size, embedding_dim, num_filters, kernel_sizes, dense_dim, category_dim)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_filters, kernel_sizes, dense_dim, category_dim, pretrained_embeddings=None):\n",
    "        super(TextCNN, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim) if pretrained_embeddings is None else nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n",
    "        self.embedding_dropout = nn.Dropout(0.3)\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=k, padding='same'),\n",
    "                nn.BatchNorm1d(num_filters),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout(0.4)\n",
    "            )\n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "\n",
    "        self.category_fc = nn.Linear(category_dim, dense_dim)\n",
    "        self.fc1 = nn.Linear(num_filters * len(kernel_sizes) + dense_dim, dense_dim)\n",
    "        self.fc2 = nn.Linear(dense_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, categories):\n",
    "        embedded = self.embedding_dropout(self.embedding(text)).permute(0, 2, 1)\n",
    "        conv_outs = [conv(embedded).max(dim=2)[0] for conv in self.convs]\n",
    "        text_features = torch.cat(conv_outs, dim=1)\n",
    "\n",
    "        category_features = F.leaky_relu(self.category_fc(categories))\n",
    "\n",
    "        combined_features = torch.cat([text_features, category_features], dim=1)\n",
    "        combined_features = self.dropout(combined_features)\n",
    "        output = self.sigmoid(self.fc2(F.leaky_relu(self.fc1(combined_features))))\n",
    "        return output\n",
    "\n",
    "# Model Parameters\n",
    "vocab_size = tokenizer.vocab_size\n",
    "embedding_dim = 100  # Reduced from 128\n",
    "num_filters = 128  # Reduced from 200\n",
    "kernel_sizes = [3, 4, 5]\n",
    "dense_dim = 64\n",
    "epochs = 20  # Increased from 15\n",
    "category_dim = len(df['feature'][0]) - 1\n",
    "\n",
    "# Initialize model\n",
    "model = TextCNN(vocab_size, embedding_dim, num_filters, kernel_sizes, dense_dim, category_dim)\n",
    "\n",
    "# Add L2 regularization\n",
    "weight_decay = 1e-5\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=weight_decay)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "# Gradient clipping\n",
    "clip_value = 1.0\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader, criterion, optimizer, epochs=15):\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_correct = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for texts, categories, labels in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(texts, categories)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            preds = (outputs >= 0.5).float()\n",
    "            train_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        train_accuracy = train_correct / total_samples\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_correct = 0\n",
    "        total_val_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for texts, categories, labels in val_dataloader:\n",
    "                outputs = model(texts, categories)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_val_loss += loss.item()\n",
    "                \n",
    "                # Calculate validation accuracy\n",
    "                preds = (outputs >= 0.5).float()\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                total_val_samples += labels.size(0)\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        val_accuracy = val_correct / total_val_samples\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"Epoch {epoch + 1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies\n",
    "\n",
    "epochs = 15\n",
    "\n",
    "# Train the model and get stats\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = train_model(\n",
    "    model, train_dataloader, val_dataloader, criterion, optimizer, epochs=epochs\n",
    ")\n",
    "\n",
    "# Plot Losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Accuracies\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: save the model\n",
    "\n",
    "import torch\n",
    "\n",
    "# Assuming 'model' is your trained model\n",
    "torch.save(model.state_dict(), 'text_cnn_model.pth')\n",
    "\n",
    "# To download the saved model file:\n",
    "from google.colab import files\n",
    "files.download('text_cnn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate on a test set and get confidence scores\n",
    "def evaluate_on_test(model, test_dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_confidences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for texts, categories, labels in test_dataloader:\n",
    "            outputs = model(texts, categories)\n",
    "            preds = (outputs >= 0.5).float()\n",
    "            confidence_scores = outputs.squeeze().tolist()  # Convert to list\n",
    "            \n",
    "            all_preds.extend(preds.squeeze().tolist())\n",
    "            all_labels.extend(labels.squeeze().tolist())\n",
    "            all_confidences.extend(confidence_scores)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return all_preds, all_labels, all_confidences\n",
    "\n",
    "# Create test DataLoader\n",
    "test_dataset = TextDataset(test_df)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Evaluate on test set\n",
    "preds, labels, confidences = evaluate_on_test(model, test_dataloader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cinf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
